{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fVNE4BGIsOlY"
   },
   "source": [
    "# Modèle d'Analyse des Sentiments basé sur un réseau de neurones\n",
    "\n",
    "L’objectif de ce TP est de développer un modèle prédictif pour la classification des sentiments des critiques de films (données textuelles) à partir d’un apprentissage automatique par réseau de neurones et d'une représentation en sac de mots (bag of words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QQ5vas82illV"
   },
   "source": [
    "## Collection de données de critiques de films \n",
    "Le tp utilisera l'ensemble de données issues de \n",
    "[Movie Review Polarity Dataset](https://www.cs.cornell.edu/people/pabo/movie-review-data/review_polarity.tar.gz) que vous devez télécharger.\n",
    "\n",
    "Ce répertoire contient 1000 revues de films, considérées comme positives (répertoire pos) et 1000 autre considérées négatives (répertoire neg). Ces revues sont au format texte.\n",
    "\n",
    "90% de ces données seront utilisées en tant que données d'entraînement et 10% seront utilisées en données de test.\n",
    "\n",
    "Ces fichiers textes sont composés de mots importants (tokens), d'espaces, de signes de ponctuations et de mots de liaisons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AWFXrAtkvcg_"
   },
   "source": [
    "---\n",
    "Si vous utisez colab, vous devez charger le répertoire txt_sentoken de l'archive à la racine de votre Drive Google.\n",
    "\n",
    "Puis il vous faut monter le répertoir drive pour qu'il soit accessible par colab.\n",
    "Exécutez le code suivant,  une clé vous sera demandée. Il vous suffit de suivre le lien, de sélectionner votre profile pour obtenir votre clé que vous copierez dans le champs prévu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 74
    },
    "colab_type": "code",
    "id": "4TodTL602bBv",
    "outputId": "cbc50963-1484-4189-b87b-b0d7d60af79c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "#bloc à exécuter si vous utilisez colab\n",
    "from google.colab import drive, files\n",
    "\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3IlL2ANmisdv"
   },
   "source": [
    "---\n",
    "## Préparation des données\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oiHdx0y1i8Bi"
   },
   "source": [
    "### Chargement et nettoyage des données\n",
    "\n",
    "Il convient pour chaque fichier texte de \n",
    "  - identifier les termes (**tokens**) entre espaces,\n",
    "  - supprimer toute ponctuation,\n",
    "  - supprimer tous les mots qui ne sont pas uniquement composés de caractères alphabétiques,\n",
    "  - supprimer tous les mots reconnus en tant que mots vides (stop words) (mots de liaison)\n",
    "  - supprimer tous les mots dont la longueur est <= 1 caractère."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SLWxuYAjxPlo"
   },
   "source": [
    "### Les bibliothèques\n",
    "La préparation des données implique de pouvoir accéder au système de fichier, ainsi qu'à la bibliothèque nltk et aux bibliothèques spécialisées dans le traitement de texte.. : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tSqBXmHvxnhF"
   },
   "outputs": [],
   "source": [
    "##import pour les fichiers et le traitement de données : \n",
    "from os import listdir\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from string import punctuation\n",
    "from collections import Counter\n",
    "##import pour les réseaux de neurones : \n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import preprocessing\n",
    "from numpy import array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "n6qZZ-VJw8Wy",
    "outputId": "1c8e7c95-f0ba-4f69-9914-c034c7f534cc"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/emmanueladam/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# le code suivant charge l'ensemble des mots non importants (en anglais)\n",
    "nltk.download(\"stopwords\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions de lectures et d'\"épurage\" de fichier texte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "by0--ITCi-xa"
   },
   "outputs": [],
   "source": [
    "def load_doc(filename)->str:\n",
    "    \"\"\"retourne le texte inclu dans le fichier filename\"\"\"\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "def clean_doc(doc)->list:\n",
    "    \"\"\"retourne la liste de mots clés inclus dans le texte doc qui ne font pas parti des stop_words\"\"\"\n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word for word in tokens if len(word) > 1]\n",
    "    return tokens\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UgALvdrLKOD3"
   },
   "source": [
    "### Tests de la récupération de mots clés d'un fichier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 125
    },
    "colab_type": "code",
    "id": "0CpapoCR0Hzs",
    "outputId": "23a01def-2b8c-4fef-e689-1289a90d2d61"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les 10 premiers mots cles de  /Users/emmanueladam/Downloads/review_polarity/txt_sentoken/neg/cv007_4992.txt\n",
      "['thats', 'exactly', 'long', 'movie', 'felt', 'werent', 'even', 'nine', 'laughs', 'nine']\n",
      "les 10 premiers mots cles de  /Users/emmanueladam/Downloads/review_polarity/txt_sentoken/pos/cv994_12270.txt\n",
      "['thriller', 'set', 'modern', 'day', 'seattle', 'marked', 'marky', 'marks', 'migration', 'good']\n"
     ]
    }
   ],
   "source": [
    "#rep = '/content/drive/My Drive/txt_sentoken'\n",
    "rep = '/Users/emmanueladam/Downloads/review_polarity/txt_sentoken'\n",
    "# mots clés d'un fichier négatif\n",
    "filename = rep + '/neg/cv007_4992.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print('les 10 premiers mots cles de ', filename)\n",
    "print(tokens[:10])\n",
    "# mots clés d'un fichier positif\n",
    "filename = rep + '/pos/cv994_12270.txt'\n",
    "text = load_doc(filename)\n",
    "tokens = clean_doc(text)\n",
    "print('les 10 premiers mots cles de ', filename)\n",
    "print(tokens[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1yo75YEIjWIT"
   },
   "source": [
    "### Construire le vocabulaire global"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Axlsvtg8jhAr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb de mots cles trouves dans les repertoires :  44525\n",
      "les 20 premiers mots cles du vocabulaire (et leur nb d'apparition dans les exemples)  : assume : 45, nothing : 717, phrase : 26, perhaps : 414, one : 4966, used : 338, first : 1624, impressions : 6, rumors : 17, hardly : 114, ever : 665, \n",
      "les 10 mots cles les plus utilises :  [('film', 7980), ('one', 4966), ('movie', 4938), ('like', 3182), ('even', 2319), ('good', 2116), ('time', 2052), ('story', 1907), ('films', 1878), ('would', 1856)]\n"
     ]
    }
   ],
   "source": [
    "def add_doc_to_vocab(filename, vocab):\n",
    "    \"\"\"cumule dans la liste vocab les mots du fichier filename \n",
    "    (1 seule occurence par mot dans vocab)\"\"\"\n",
    "    # load doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # update counts\n",
    "    vocab.update(tokens)\n",
    "\n",
    "def build_voc(directory, vocab):\n",
    "    \"\"\"ajoute au dictionnaire vocab les mots cles des 900 premiers fichiers du repertoire directory\"\"\"\n",
    "    i=0\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        if i>900: break\n",
    "        # create the absolute filename\n",
    "        path = directory + '/' + filename\n",
    "        # add doc to vocab\n",
    "        add_doc_to_vocab(path, vocab)\n",
    "        i = i +1\n",
    "\n",
    "    \n",
    "# creer un vocabulaire (liste de mots clés associés à leurs occurences)\n",
    "vocab = Counter()\n",
    "# ajouter les mots cles des repertoire pos et neg\n",
    "build_voc(rep + '/pos', vocab)\n",
    "build_voc(rep + '/neg', vocab)\n",
    "# afficher le nb de mots cles trouves\n",
    "print(\"nb de mots cles trouves dans les repertoires : \", len(vocab))\n",
    "# afficher les 20 premiers mots du vocabulaire et leurs nb d'apparitions dans les exemples d'entrainement\n",
    "print(\"les 20 premiers mots cles du vocabulaire (et leur nb d'apparition dans les exemples)  : \", end='')\n",
    "i=0\n",
    "for (mot,count) in vocab.items(): \n",
    "    print(mot,':',count,end=\", \")\n",
    "    i = i+1\n",
    "    if i>10:break\n",
    "# afficher les 10 mots cles les plus utilises\n",
    "print(\"\\nles 10 mots cles les plus utilises : \", vocab.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EDFdsl97jq0t"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "en otant les mots utilise moins de  3  fois, nb de mots cles =  19665\n"
     ]
    }
   ],
   "source": [
    "# ne garder que les mots clés apparaissant au moins 3 fois\n",
    "min_occurrence = 3\n",
    "tokens = [token for (token,count) in vocab.items() if count >= min_occurrence]\n",
    "print('en otant les mots utilise moins de ', min_occurrence, ' fois, nb de mots cles = ',len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ApMJ_0gHjuEv"
   },
   "outputs": [],
   "source": [
    "def save_list(lines, filename):\n",
    "    \"\"\"sauve les mots de la liste lines dans le fichier filename\"\"\"\n",
    "    # convert lines to a single blob of text\n",
    "    data = '\\n'.join(lines)\n",
    "    # open file\n",
    "    file = open(filename, 'w')\n",
    "    # write text\n",
    "    file.write(data)\n",
    "    # close file\n",
    "    file.close()\n",
    "\n",
    "# save tokens to a vocabulary file\n",
    "save_list(tokens, 'vocab.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2oK8x7bj5CU"
   },
   "source": [
    "---\n",
    "## Représentation en sac de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wdd9RpBskGUy"
   },
   "source": [
    "### Convertir les fichiers en listes de mots-clés appartenant au vocabulaire \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Sw6xGMHkUP1"
   },
   "outputs": [],
   "source": [
    "def doc_to_line(filename, vocab)->list:\n",
    "    \"\"\"retourne la liste des mots cles du fichier filename appartenant au vocabulaire vocab\"\"\"\n",
    "    # load the doc\n",
    "    doc = load_doc(filename)\n",
    "    # clean doc\n",
    "    tokens = clean_doc(doc)\n",
    "    # filter by vocab\n",
    "    tokens = [token for token in tokens if token in vocab]\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bE7XaHdka1O"
   },
   "outputs": [],
   "source": [
    "def process_docs(directory, vocab)->list:\n",
    "    \"\"\"retourne deux listes des mots cles du repertoire directory; \n",
    "    la 1ere liste represente les 900 premiers fichiers du repertoire, la 2nde represente les 100 derniers\"\"\"\n",
    "    lines_firts = list()\n",
    "    lines_lasts = list()\n",
    "    i=1\n",
    "    # walk through all files in the folder\n",
    "    for filename in listdir(directory):\n",
    "        # create the absolute filename\n",
    "        path = directory + '/' + filename\n",
    "        # load and clean the doc\n",
    "        line = doc_to_line(path, vocab)\n",
    "        if (i<=900): lines_firts.append(line)\n",
    "        else:lines_lasts.append(line)\n",
    "        i = i+1\n",
    "    return (lines_firts,lines_lasts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0ixOikAkfyN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb exemples d'entrainement positifs :  900\n",
      "nb exemples d'entrainement negatifs :  900\n",
      "nb exemples de tests positifs :  100\n",
      "nb exemples de tests negatifs :  100\n"
     ]
    }
   ],
   "source": [
    "# load the vocabulary\n",
    "vocab_filename = 'vocab.txt'\n",
    "vocab = load_doc(vocab_filename)\n",
    "vocab = vocab.split()\n",
    "vocab = set(vocab)\n",
    "# load training and testing reviews\n",
    "(positive_lines_train, positive_lines_test) = process_docs(rep+'/pos', vocab)\n",
    "(negative_lines_train, negative_lines_test) = process_docs(rep+'/neg', vocab)\n",
    "# summarize what we have\n",
    "print(\"nb exemples d'entrainement positifs : \", len(positive_lines_train))\n",
    "print(\"nb exemples d'entrainement negatifs : \", len(negative_lines_train))\n",
    "print(\"nb exemples de tests positifs : \", len(positive_lines_test))\n",
    "print(\"nb exemples de tests negatifs : \", len(negative_lines_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZTJ5fim2k1fk"
   },
   "source": [
    "\n",
    "### Convertir les listes de mots en vecteur fréquence d'apparition \n",
    "#### Créer le sac de mot de l'ensemble d'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zVxIx3dtk9HF"
   },
   "outputs": [],
   "source": [
    "# create the tokenizer\n",
    "tokenizer = keras.preprocessing.text.Tokenizer()\n",
    "# load the positive and negative train set into the tokenizer\n",
    "docs = positive_lines_train + negative_lines_train\n",
    "tokenizer.fit_on_texts(docs)\n",
    "# ask to the tokenizer to build the bag of words : a set of (word, frequence of use)*\n",
    "Xtrain = tokenizer.texts_to_matrix(docs, mode='freq')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SGd85sb5k_Xs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtrain contient  1800  exemples de  19663  valeurs de fréquence d'apparition des mots des exemples.\n",
      "Ainsi, premier exemple d'entrainement = \n",
      " [0.         0.00477327 0.01431981 ... 0.         0.         0.        ]\n"
     ]
    }
   ],
   "source": [
    "print('Xtrain contient ', Xtrain.shape[0], ' exemples de ', Xtrain.shape[1], ' valeurs de fréquence d\\'apparition des mots des exemples.')\n",
    "print('Ainsi, premier exemple d\\'entrainement = \\n', Xtrain[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On décide que les exemples positifs correspondent à une sortie 0 et que les exemples négatifs correspondent à la sortie 1 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ytrain = suites de 0 (classement pour eval positive), suivis d'une suite de 1 (classements pour éval négative)\n",
    "ytrain = array([0 for _ in range(len(positive_lines_train))] + [1 for _ in range(len(negative_lines_train))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créer le sac de mot de l'ensemble de test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "twHl-RSElEB_"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Xtest contient  200  exemples de  19663  valeurs de fréquence.\n"
     ]
    }
   ],
   "source": [
    "# tokenize the lines of the test sample\n",
    "docs = positive_lines_test + negative_lines_test\n",
    "# ask to the tokenizer to give the bag of words : a set of (word, frequence of use),\n",
    "# the words are already kown by the tokenizer*\n",
    "Xtest = tokenizer.texts_to_matrix(docs, mode='freq')\n",
    "print('Xtest contient ', Xtest.shape[0], ' exemples de ', Xtest.shape[1], ' valeurs de fréquence.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sortie attendues des exemples de test, ytest = suites de 0, suivis d'une suite de 1\n",
    "ytest = array([0 for _ in range(len(positive_lines_test))] + [1 for _ in range(len(negative_lines_test))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCJRY1mTlnls"
   },
   "source": [
    "---\n",
    "## Modèle de réseau pour l'analyse des sentiments\n",
    "Le réseau contient en couche d'entrée aurant de neurones que de valeurs retenues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uruPxsQil-gZ"
   },
   "outputs": [],
   "source": [
    "#TODO: donnez le nb de neurones en entrée (= nb de tokens retenus)\n",
    "n_words = ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "67M4JqX2lNeu"
   },
   "outputs": [],
   "source": [
    "# TODO: définir la structure du réseau\n",
    "model = Sequential()\n",
    "# tester différents nb de couches, composés de divers neurones\n",
    "# tester différentes fonction d'activation (sigmoid, tanh, elu, relu, ...), \n",
    "# -> cf. https://keras.io/activations/\n",
    "model.add(Dense(...\n",
    "\n",
    "model.add(Dense(...\n",
    "\n",
    "# compiler le reseau et définir l'optimiseur et la méthode de calcul d'erreur\n",
    "# tester différents calcul de l'errreur (loss) : mean_squared_error, binary_crossentropy, ..\n",
    "# -> cf. https://keras.io/losses/\n",
    "# tester différents optimizer : sgd, adam, adagrad, ...\n",
    "# -> cf. https://keras.io/optimizers/\n",
    "\n",
    "model.compile(...)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rlYk8aKemF9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "1800/1800 - 4s - loss: 1.0082 - accuracy: 0.5033\n",
      "Epoch 2/8\n",
      "1800/1800 - 4s - loss: 0.5820 - accuracy: 0.8839\n",
      "Epoch 3/8\n",
      "1800/1800 - 3s - loss: 0.3947 - accuracy: 0.9261\n",
      "Epoch 4/8\n",
      "1800/1800 - 4s - loss: 0.1566 - accuracy: 0.9772\n",
      "Epoch 5/8\n",
      "1800/1800 - 4s - loss: 0.0471 - accuracy: 0.9956\n",
      "Epoch 6/8\n",
      "1800/1800 - 4s - loss: 0.0134 - accuracy: 1.0000\n",
      "Epoch 7/8\n",
      "1800/1800 - 4s - loss: 0.0035 - accuracy: 1.0000\n",
      "Epoch 8/8\n",
      "1800/1800 - 4s - loss: 0.0013 - accuracy: 1.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x104f82ac8>"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#TODO tester differents nb de tests (epochs)\n",
    "model.fit(Xtrain, ytrain, epochs=8, verbose=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KVnJJYscmH4v"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 85.000002\n"
     ]
    }
   ],
   "source": [
    "# evaluate\n",
    "loss, acc = model.evaluate(Xtest, ytest, verbose=0)\n",
    "print('Test Accuracy: %f' % (acc*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObKC8oW7mROA"
   },
   "source": [
    "---\n",
    "## Estimation de nouvelles critiques\n",
    "Il ne reste plus qu'à utiliser le réseau pour faciliter la classification de futures revues ..  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D3A6k30XmVxr"
   },
   "outputs": [],
   "source": [
    "def predict_sentiment(review, vocab, tokenizer, model):\n",
    "    \"\"\"classifie le texte dans la variable 'review' en positif (0) ou negatif (1) \"\"\"\n",
    "    # clean\n",
    "    tokens = clean_doc(review)\n",
    "    # filter by vocab\n",
    "    tokens = [w for w in tokens if w in vocab]\n",
    "    # convert to line\n",
    "    line = ' '.join(tokens)\n",
    "    # create the bag of words with the words used in the line\n",
    "    encoded = tokenizer.texts_to_matrix([line], mode='freq')\n",
    "    # prediction, output is a vector of (1x1) here (output of the neural network is a vector of 1 element)\n",
    "    output = model.predict(encoded, verbose=0)\n",
    "    estimation = round(output[0,0])\n",
    "    return estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VslEKt3WmYuI"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "' This movie is the beginning of the culmination of Marvel's masterfully woven cinematic universe. Beginning back in 2008 with iron man, we are finally seeing the results of all the movies have been pointing to; and it did not disappoint. Thanos is a complex villain, with deeper and more interesting desires than just \"world domination.\" The dilemmas all the characters face in this movie (both the heroes and the villains) are truly thought provoking and leave you on the edge of your seat. No other set of movies has beeen so involved, so expanded, and encompassed so many story lines/characters and previous movies. The sheer amount of star power alone in this film is insane; and they do a masterful job of weaving all these unique and various characters into a common storyline. ' ------> est considere comme  positif\n",
      "' I was so angry after watching that I had to write something. Boring boring boring, the biggest reason for that is scenario 0/10. It is too long, if I hadn't go to cinema and bought ticket there is no way I would have patience to watch it until the end. Everything already seen in previous marvel movies, it seems that every new is even worse with lower quality and I didn't even have big expectations before the movie. The main reason I decided to watch it is IMDB rate 9/10 which is total fraud and paid advertisement. There is no guarantee if some character is dead that he will remain dead, everyone could resurrect...Go watch anything else than this. My last marvel movie for sure, so angry that I don't watch american movies in cinema any more, all the same, low quality in recent time, just to make many fast, without proper scenario...bull..it ' ------> est considere comme  negatif\n"
     ]
    }
   ],
   "source": [
    "# test of a review associated with a note of 10/10\n",
    "text = 'This movie is the beginning of the culmination of Marvel\\'s masterfully woven cinematic universe. Beginning back in 2008 with iron man, we are finally seeing the results of all the movies have been pointing to; and it did not disappoint. Thanos is a complex villain, with deeper and more interesting desires than just \"world domination.\" The dilemmas all the characters face in this movie (both the heroes and the villains) are truly thought provoking and leave you on the edge of your seat. No other set of movies has beeen so involved, so expanded, and encompassed so many story lines/characters and previous movies. The sheer amount of star power alone in this film is insane; and they do a masterful job of weaving all these unique and various characters into a common storyline.'\n",
    "resultat = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('\\'',text,'\\' ------> est considere comme ', 'positif' if resultat==0 else 'negatif')\n",
    "\n",
    "# test of a review associated with a note of 1/10\n",
    "text = 'I was so angry after watching that I had to write something. Boring boring boring, the biggest reason for that is scenario 0/10. It is too long, if I hadn\\'t go to cinema and bought ticket there is no way I would have patience to watch it until the end. Everything already seen in previous marvel movies, it seems that every new is even worse with lower quality and I didn\\'t even have big expectations before the movie. The main reason I decided to watch it is IMDB rate 9/10 which is total fraud and paid advertisement. There is no guarantee if some character is dead that he will remain dead, everyone could resurrect...Go watch anything else than this. My last marvel movie for sure, so angry that I don\\'t watch american movies in cinema any more, all the same, low quality in recent time, just to make many fast, without proper scenario...bull..it'\n",
    "resultat = predict_sentiment(text, vocab, tokenizer, model)\n",
    "print('\\'',text,'\\' ------> est considere comme ', 'positif' if resultat==0 else 'negatif')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "## TRAVAIL A RENDRE\n",
    "\n",
    "* Vous devrez tester différentes architectures de réseau, ainsi que différentes configuration (fonctions d'activation, méthodes de correction d'erreur, méthode de calcul de l'erreur, ...)\n",
    "* Vous enverrez un fichier contenant pour chaque définition de réseau :\n",
    "    * l'architecture du réseau (un copier coller du bloc 'TODO: définir la structure du réseau')\n",
    "    * le déroulé de l'apprentissage (un copier coller du bloc résultat de 'TODO tester differents nb de tests (epochs)'\n",
    "    * le résultat de 'Test Accuracy'\n",
    "\n",
    "Au minimum 3 différents réseaux devront être fournis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copie de TP_DeepLearningM1TNSI.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
