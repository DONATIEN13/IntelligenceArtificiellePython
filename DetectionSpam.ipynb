{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TestOUX.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3qF8xmpAdACr"
   },
   "source": [
    "# Détection de SPAM par réseaux de neurones\n",
    "\n",
    "Dans cet exercice, le but est de créer un systèmes très basique de détection de neurones par Réseaux de Neurones en se basant uniquement sur des mots clés trouvés dans les sujets de mails.\n",
    "\n",
    "Une version plus évoluée étudierait aussi le texte transormé en sac de mots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "*L’observation du contenu des communications par les gouvernements dans un but de sécurité permettrait de détecter les mails potentiellement dangereux et de lever des alertes.\n",
    "Cependant la masse d’information échangée est trop importante pour garantir une bonne analyse.\n",
    "Des réseaux de neurones peuvent être utilisés pour guider la détection de problèmes potentiels.*\n",
    "\n",
    "*Il s’agit donc ici de définir un réseau de neurones et de lui faire apprendre à détecter des textes à risque.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les exemples d’apprentissage seront constitués :\n",
    "- d’une série de valeurs indiquant si oui ou non un terme est présent dans le texte analysé,\n",
    "- de la réponse attendue (dans la ligne danger)\n",
    "\n",
    "Voici les exemples d’apprentissage :\n",
    "\n",
    "| bombe | feu | cheminée | fille | canon | mec | acide | chlorhydrique | kalash | clip | balle | tennis | danger |\n",
    "|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|:--:|\n",
    "| 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
    "| 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
    "| 0 | 1 | 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
    "| 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 1 |\n",
    "| 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 | 0 | 0 |\n",
    "| 0 | 0 | 0 | 1 | 0 | 0 | 1 | 0 | 0 | 0 | 0 | 0 | 1 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 0 | 1 | 1 | 1 |\n",
    "| 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 1 | 0 | 0 | 0 |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JZpJB9iOdRS3"
   },
   "source": [
    "---\n",
    "## Importer les librairires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odDg9S7xdFsv"
   },
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XN8WXXdTdXx0"
   },
   "source": [
    "---\n",
    "\n",
    "### Définir les entrées et sorties attendues\n",
    "\n",
    "Les entrées correspondent aux présente des mots clés par sujet de mail.\n",
    "Ici, les entrées d'apprentissage sont consituées de 70% de l'exemple présenté (donc 9 lignes de  13 valeurs 0,1). Les sorties d'apprentissage correspondent au fait que chaque entrée soit un spam ou non.\n",
    "\n",
    "Les entrées et sorties de validations sont donc constitués des 40% restants\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "A1mjGN2Bb5Ki"
   },
   "outputs": [],
   "source": [
    "#définissez les entrées sorties sur base du tableau\n",
    "entrees_app = np.array(... , float)\n",
    "sorties_app = np.array(... , float)\n",
    "entrees_val = np.array(... , float)\n",
    "sorties_val = np.array(... , float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SC-1MnShdwe0"
   },
   "source": [
    "---\n",
    "\n",
    "### Choisir le modèle de réseau\n",
    "- ici les couches sont séquentielles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WbST4EmqcCdJ"
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jo5Ej8kkd8nh"
   },
   "source": [
    "---\n",
    "\n",
    "### Définir l'architecture du réseau\n",
    "Choisissez la struture du réseau, le nb de couches cachées, etc.\n",
    "- une première couche composée de \n",
    "  - 13 neurones en entrée , plus le neurone BIAS (ou non)\n",
    "  - x neurones en sortie \n",
    "- une couche composée\n",
    "  - y neurones en entrée (ceux de la couche précédente) et \n",
    "  - de 1 neurone en sortie (Spam ou non)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vr6dcOJicGYM"
   },
   "outputs": [],
   "source": [
    "avec_bias = True\n",
    "#une premiere couche constituée de 13 neurones en entrée, ...\n",
    "model.add(Dense(...))\n",
    "\n",
    "#une derniere couche constituée de 1 neurone en sortie \n",
    "model.add(Dense(1, activation='sigmoid'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8L4LxSBWeaF1"
   },
   "source": [
    "---\n",
    "\n",
    "### Compiler le  réseau\n",
    "- ici, on précise que l'algo de correction d'erreur est *'adam'*, et que l'erreur calculée est la moyenne des valeurs absolues des erreurs commises\n",
    "(vous pouvez chosir un autre optimizer et un autre clacul de loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HlnG97g7cQKW"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='MSE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rZ4IQ-bIdtN1"
   },
   "source": [
    "---\n",
    "\n",
    "### Entraîner le réseau \n",
    "- ici on  le fait  'parler' (verbose=1), et on lance xx cycles d'apprentissage (epochs à préciser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 148
    },
    "colab_type": "code",
    "id": "ddTla-J_cfz8",
    "outputId": "0fd3cade-cd59-41db-c5f6-0ea22e48cac0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x6468cd450>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tests = model.fit(entrees_app, sorties_app, \n",
    "                    validation_data=(entrees_cal, sorties_val),\n",
    "                    epochs=??, verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I0zBYIALlDYp"
   },
   "source": [
    "---\n",
    "\n",
    "## Dessiner l'évolution de l'erreur et de la pertinence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z89vptAXcuDM"
   },
   "outputs": [],
   "source": [
    "history_dict = tests.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tracer l'erreur d'entrainement / l'erreur de validation\n",
    "- L'erreur d'entrainement et l'erreur de validation doivent suivre des courbes quasi parallèle.\n",
    "- L'erreur d'entrainement doit être plus faible que l'erreur de validation\n",
    "    - si les courbes divergent, si la courbe de validation s'éloigne de la courbe d'entrainement, il y a alors un sur-apprentissage (overfitting) trop adapté aux données d'entrainement, sans doute trop poussé (diminuer les epochs?).\n",
    "    - s'il y a une erreur d'entrainement trop grande, il y a sous-apprentissage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# draw the loss evolution in blue\n",
    "plt.plot(epochs, loss, 'b-*', label='erreur sur exemples d\\'apprentissage')\n",
    "# draw the accuracy evolution in blue\n",
    "plt.plot(epochs, val_loss, 'r-*', label='erreur sur exemples de validation')\n",
    "plt.title('Erreur')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Erreur')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(loss) + 1)\n",
    "\n",
    "# draw the loss evolution in blue\n",
    "plt.plot(epochs, acc, 'b-*', label='précision sur exemples d\\'apprentissage')\n",
    "# draw the accuracy evolution in blue\n",
    "plt.plot(epochs, val_acc, 'r-*', label='précision sur exemples de validation')\n",
    "plt.title('précision')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('précision')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N38spzckmMMJ"
   },
   "source": [
    "---\n",
    "## Utilisation du réseau\n",
    "Quelle est la probabilité que les textes contenant les mots :\n",
    "1. clip acide\n",
    "2. cheminée balle\n",
    "3. mec fille canon\n",
    "4. kalash bombe\n",
    "soient des textes suspects ?\n",
    "\n",
    "On détecte alors que le texte contenant les mots « clip » et « acide » est suspect à plus de 95% et que le texte contenant les mots « bombe » et « kalash » ne l’est qu’à 45%.\n",
    "\n",
    "Ajouter les lignes aux exemples d’entrainement permettant d’ôter le premier texte de la ligne des suspects et d’ajouter le second dans cette liste de suspects.\n",
    "\n",
    "Relancer l’apprentissage du réseau et vérifier que les textes sont bien classés."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "TestOUX.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
