{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TPMLGymQLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Appliqué à [Gym.OpenAI](https://gym.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Présentation de Gym\n",
    "\n",
    "Voir la page d'introduction à [Gym](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation de gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outil AUTRE QUE COLAB (pyzo, jupyter lab, .....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test de ML par Q-Learning pour atteindre l'objectif\n",
    "- Utiliser l'environnement `FrozenLake8x8-v0` (un labyrinthe en mode texte)\n",
    "- 4 actions sont possibles (Left(0), Down(1), Right(2), Up(3))\n",
    "  - l'adjectif \"Frozen\" signifie qu'une *action n'est pas déterministe !*\n",
    "    - à partir d'une case \"gelée\", aller à droite peut .. mener à droite, ou pas\n",
    "    - => intérêt du Q-Learning adapté à ce type d'environnement probabiliste\n",
    "- Le labyrinthe est ainsi composé de zones glacées, de puits, et d'un objectif\n",
    "\n",
    "En vous basant sur les codes présent dans l'article [Q-Learning en Java](http://emmanuel.adam.free.fr/site/spip.php?article134), programmer un algo de Q-Learning pour apprendre à atteindre l'objectif \n",
    "\n",
    "**N.B.** \n",
    "  - *Cet environnement fonctionne bien sous colab, jupyterlab.. quelques soucis de l'affichage de l'état courant (carré rouge) sous Pyzo....* \n",
    "  - Il est fortement conseillé de débuter avec un environnement déterministe pour évaluer la bonne marche de l'algo de Q-Learning que vous aurez développer..\n",
    "    - pour cela, lancer l'environnement avec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Etude de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(FrozenLake-v0)\n",
      "Discrete(4)\n",
      "Discrete(16)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "S = Start, G = Goal, H = Hole, F = Frozen place\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0') # tester FrozenLake8x8 pour l'environnement plus large\n",
    "print(env.spec)\n",
    "print(env.action_space) #ici 4 actions discrétisée\n",
    "print(env.observation_space) # ici 4x4 cellules possibles\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "print(\"S = Start, G = Goal, H = Hole, F = Frozen place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Test des actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "###### Test des fonctions\n",
    "env.reset()\n",
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 4 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 4 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 3\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note : \n",
    "## observation = position où se trouve l'agent\n",
    "## reward = recompense\n",
    "## done = but atteint\n",
    "## info = proba de reussite de l'action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est clairement ici dans un environnement non déterministe (une même action à partir d'un même état ne mène pas toujours au même résultat); c'est le contexte de prédilection de l'algo de Q-Learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=\"red\">Mise en place de l'environnement en mode déterministe</font>\n",
    "Important, pour valider l'apprentissage de votre algorithme avant de passer en mode non-déterministe, il vaut mieux le tester sur un environnement où chaque action à 100% de réussite. Ci-dessous un exemple sur le mini labyrinthe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 6 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "pos° actuelle: 14 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "pos° actuelle: 15 ,gain: 1.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "***Aide Python : (plusieurs possibilités)***\n",
    "- Il n'est pas nécessaire de créer des classes Etat, ....\n",
    "- Il faut pouvoir stocker les récompenses de chaque actions à partir de chaque case\n",
    "  - a priori, la création d'une matrice 4x4 de 4 valeurs peut être utile.\n",
    "  Plus simplement, on utilisera un tableau de n cases (pour les n etats) contenant m valeurs d'actions \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# cree un tableau de n cases x m actions\n",
    "q_actions = np.zeros((env.observation_space.n, env.action_space.n), np.float32)\n",
    "print(q_actions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Recherche du max et de sa position*\n",
    "\n",
    "Reprenons la derniere modélisation sous forme de tableau de 4 valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2066786  0.09571378 0.51688347 0.05415255]\n",
      " [0.75066335 0.74831572 0.06387416 0.84854258]\n",
      " [0.83406533 0.80558654 0.22978986 0.23528311]\n",
      " [0.44997395 0.66794793 0.61921367 0.48222429]\n",
      " [0.85204815 0.30694744 0.78731277 0.11334462]\n",
      " [0.2733148  0.79489722 0.83269927 0.4747544 ]\n",
      " [0.93406451 0.28838276 0.59015805 0.67367305]\n",
      " [0.18213117 0.95138439 0.40462532 0.84738508]\n",
      " [0.48898813 0.55316361 0.5107132  0.11692906]\n",
      " [0.21276702 0.26798434 0.65192879 0.76056712]\n",
      " [0.86129877 0.10571659 0.02035535 0.29236254]\n",
      " [0.25580737 0.99092468 0.69634389 0.58834477]\n",
      " [0.02926987 0.76848053 0.7319501  0.62312869]\n",
      " [0.22711828 0.77157755 0.81645795 0.35623859]\n",
      " [0.68139813 0.87213529 0.37016551 0.89845041]\n",
      " [0.64915572 0.64189817 0.70991113 0.71377006]\n",
      " [0.76090114 0.45779089 0.28186897 0.97592355]\n",
      " [0.20407737 0.23451658 0.99375917 0.12687101]\n",
      " [0.95511916 0.70910822 0.41400826 0.70365044]\n",
      " [0.05508487 0.32477776 0.24022979 0.61247974]\n",
      " [0.98278188 0.47244426 0.31157547 0.1859517 ]\n",
      " [0.34801596 0.77821002 0.1730638  0.87487545]\n",
      " [0.21152903 0.16844717 0.56747992 0.66922174]\n",
      " [0.4530878  0.89297505 0.58162704 0.18378944]\n",
      " [0.57795546 0.68866233 0.82559799 0.86639968]\n",
      " [0.27469204 0.62945688 0.75978789 0.47133242]\n",
      " [0.96426743 0.47810456 0.46761953 0.02783484]\n",
      " [0.52896519 0.07243694 0.33011565 0.95393714]\n",
      " [0.35434496 0.66430425 0.50112318 0.66021193]\n",
      " [0.77993313 0.17063211 0.35685096 0.12431831]\n",
      " [0.59365858 0.77332629 0.15063122 0.39556105]\n",
      " [0.76124217 0.80705321 0.01907173 0.53152891]\n",
      " [0.24541001 0.52930362 0.11376302 0.38549232]\n",
      " [0.24552018 0.74218181 0.49491167 0.49788271]\n",
      " [0.22380623 0.80933859 0.88219491 0.36044129]\n",
      " [0.93429178 0.6303876  0.87383348 0.0268235 ]\n",
      " [0.95174891 0.54998406 0.83782159 0.30794658]\n",
      " [0.68579167 0.82136119 0.6060241  0.19284737]\n",
      " [0.75263571 0.78892225 0.4359237  0.45202226]\n",
      " [0.43576744 0.5441226  0.83293969 0.4745995 ]\n",
      " [0.55740682 0.98378163 0.97101007 0.927394  ]\n",
      " [0.08654054 0.07073994 0.07775578 0.28569301]\n",
      " [0.9871419  0.90808459 0.68913262 0.34874295]\n",
      " [0.64399774 0.17908542 0.6651278  0.80774498]\n",
      " [0.31270718 0.80964233 0.53567752 0.9539076 ]\n",
      " [0.76473514 0.17942788 0.02728933 0.75846031]\n",
      " [0.68803572 0.0090343  0.30099751 0.64721664]\n",
      " [0.06189205 0.63342903 0.77112036 0.27207557]\n",
      " [0.19942524 0.39914242 0.36003807 0.06698385]\n",
      " [0.61917476 0.34599064 0.49472977 0.75961926]\n",
      " [0.73079232 0.17882905 0.40827809 0.55366455]\n",
      " [0.24535749 0.22093839 0.53183195 0.36728125]\n",
      " [0.78859969 0.77580564 0.94680541 0.44922384]\n",
      " [0.21541726 0.14846705 0.5005431  0.27637378]\n",
      " [0.42392259 0.20781325 0.56602325 0.75020581]\n",
      " [0.57774141 0.58378962 0.3203917  0.61703382]\n",
      " [0.34548789 0.0522559  0.76355603 0.29983529]\n",
      " [0.0780777  0.0718217  0.64281151 0.80268961]\n",
      " [0.08066079 0.20043549 0.59591627 0.64948051]\n",
      " [0.51700512 0.90003822 0.94779382 0.24788951]\n",
      " [0.83882531 0.55212243 0.44697318 0.75004865]\n",
      " [0.99679996 0.22922655 0.22764356 0.76193955]\n",
      " [0.46487143 0.53909394 0.53466688 0.90983282]\n",
      " [0.56355165 0.61871598 0.55466078 0.14261718]]\n"
     ]
    }
   ],
   "source": [
    "#Exemple de recherche..\n",
    "#1. ici pour l'exemple, on place des valeurs aléatoires pour les actions\n",
    "Q = np.random.random((env.observation_space.n, env.action_space.n))\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 2, max= 0.8340653280315182 , en position  0\n",
      "la meilleure action de l'etat 2 serait donc  0\n"
     ]
    }
   ],
   "source": [
    "#max de la case 2\n",
    "max_2 = np.max(Q[2, :])\n",
    "position_max_2 = np.argmax(Q[2,:])\n",
    "print(\"case 2, max=\",max_2, \", en position \", position_max_2)\n",
    "print(\"la meilleure action de l'etat 2 serait donc \", position_max_2)"
   ]
  },
 
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {}, 
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
