{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TPMLGymQLearning-Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Appliqué à [Gym.OpenAI](https://gym.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Présentation de Gym\n",
    "\n",
    "Voir la page d'introduction à [Gym](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation de gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outil AUTRE QUE COLAB (pyzo, jupyter lab, .....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test de ML par Q-Learning pour atteindre l'objectif\n",
    "- Utiliser l'environnement `FrozenLake8x8-v0` (un labyrinthe en mode texte)\n",
    "- 4 actions sont possibles (Left(0), Down(1), Right(2), Up(3))\n",
    "  - l'adjectif \"Frozen\" signifie qu'une *action n'est pas déterministe !*\n",
    "    - à partir d'une case \"gelée\", aller à droite peut .. mener à droite, ou pas\n",
    "    - => intérêt du Q-Learning adapté à ce type d'environnement probabiliste\n",
    "- Le labyrinthe est ainsi composé de zones glacées, de puits, et d'un objectif\n",
    "\n",
    "En vous basant sur les codes présent dans l'article [Q-Learning en Java](http://emmanuel.adam.free.fr/site/spip.php?article134), programmer un algo de Q-Learning pour apprendre à atteindre l'objectif \n",
    "\n",
    "**N.B.** \n",
    "  - *Cet environnement fonctionne bien sous colab, jupyterlab.. quelques soucis de l'affichage de l'état courant (carré rouge) sous Pyzo....* \n",
    "  - Il est fortement conseillé de débuter avec un environnement déterministe pour évaluer la bonne marche de l'algo de Q-Learning que vous aurez développer..\n",
    "    - pour cela, lancer l'environnement avec "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Etude de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EnvSpec(FrozenLake-v0)\n",
      "Discrete(4)\n",
      "Discrete(16)\n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "S = Start, G = Goal, H = Hole, F = Frozen place\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0') # tester FrozenLake8x8 pour l'environnement plus large\n",
    "print(env.spec)\n",
    "print(env.action_space) #ici 4 actions discrétisée\n",
    "print(env.observation_space) # ici 4x4 cellules possibles\n",
    "\n",
    "env.reset()\n",
    "env.render()\n",
    "print(\"S = Start, G = Goal, H = Hole, F = Frozen place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Test des actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 4 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "###### Test des fonctions\n",
    "env.reset()\n",
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 3\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#note : \n",
    "## observation = position où se trouve l'agent\n",
    "## reward = recompense\n",
    "## done = but atteint\n",
    "## info = proba de reussite de l'action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est clairement ici dans un environnement non déterministe (une même action à partir d'un même état ne mène pas toujours au même résultat); c'est le contexte de prédilection de l'algo de Q-Learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=\"red\">Mise en place de l'environnement en mode déterministe</font>\n",
    "Important, pour valider l'apprentissage de votre algorithme avant de passer en mode non-déterministe, il vaut mieux le tester sur un environnement où chaque action à 100% de réussite. Ci-dessous un exemple sur le mini labyrinthe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 6 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "pos° actuelle: 14 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "pos° actuelle: 15 ,gain: 1.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exemple de solution\n",
    "### en mode non déterministe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True)\n",
    "actions = {0:'Gauche', 1:'Bas', 2:'Droite', 3:'Haut'}\n",
    "\n",
    "# initialiser la Q-Table\n",
    "# autant de case que l'environnement en possede, \n",
    "# contanant autant de valeurs que d'actions possibles\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en place des paramètres\n",
    "Pour rappel l'algo de Q Learning simple repose sur cette équation : \n",
    "$Q(s,a) \\gets \\lambda \\times (r + \\gamma \\times max_{a'}(Q(s', a'))) + (1-\\lambda ) \\times Q(s,a)$ avec \n",
    "  - $\\lambda$ : coef d'apprentissage\n",
    "  - $\\gamma$ : coef de réduction\n",
    "  - $r$ : récompense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_learn = .8\n",
    "gamma = 0.99\n",
    "nb_episodes = 50000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- L'algo va tout d'abord tatonner, choisir des actions aléatoirement, modifier la valeur de l'action choisie en fonction de son résultat en espérant atteindre l'objectif au bout d'un certain nombre de tests.\n",
    "- Il va recommencer quelques milliers de fois, en tatonnant de moins en moins et en prenant de plus en plus en considération la valeur des actions afin de les choisir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.00788014, -0.46340962, -1.83357718,  1.50470343]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Q étant la table des actions par case\n",
    "# Q[2,:] retourne le tableau des valeurs des actions de la case 2\n",
    "# np.argmax(Q[2,:]) retourne le no de l'action ayant la plus grande utilité\n",
    "# np.random.randn(1,env.action_space.n) retourne un tableau de 4 valeurs entre 0 et 1\n",
    "np.random.randn(1,env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#np.argmax(Q[2,:]+np.random.randn(1,env.action_space.n)) \n",
    "#    retourne l'action ayant la plus grande utilité après ajout de bruit\n",
    "np.argmax(Q[2,:]+np.random.randn(1,env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'idée est alors d'ajouter de moins en moins de bruit au fur et à mesure des tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme de Q-Learning simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##algorithme de Q-Learning simple\n",
    "def Qlearn(epoch):\n",
    "    \"\"\"\n",
    "    effectue un cycle d'apprentissage/recherche de solution' via le Q-Learning simple\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : no de l'etape\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_r : recompense totale\n",
    "    r : recompense du dernier etat rencontre\n",
    "    states_list : liste des etats traverses\n",
    "    actions_list : liste des actions effectuees\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_reward = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    # liste des etats\n",
    "    states_list = []\n",
    "    # liste des actions\n",
    "    actions_list = []\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done and step < 25:\n",
    "        step += 1\n",
    "        # Choix d'une action avec bruitage de moins en moins marque\n",
    "        a = np.argmax(Q[s, :] + np.random.randn(1, env.action_space.n) * (5. / (epoch + 1)))\n",
    "\n",
    "        # recuperer un nouvel etat, sa recompense et son etat\n",
    "        new_state, r, done, _ = env.step(a)\n",
    "\n",
    "        # bouger coute de l'energie\n",
    "        if r != 1: r = r - 0.001\n",
    "\n",
    "        # Q-Learning\n",
    "        Q[s, a] = Q[s, a] + lambda_learn * (r + gamma * np.max(Q[new_state, :]) - Q[s, a])\n",
    "        s = new_state\n",
    "        total_reward = total_reward + r\n",
    "        states_list.append(s)\n",
    "        actions_list.append(a)\n",
    "    return total_reward, r, states_list, actions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_qlearn():\n",
    "    \"\"\"\n",
    "    lance nb_episodes fois un cycle de Q-Learning et memorise chaque solution trouvee\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    solutions_list : liste des solutions (no, recompense totale, liste des etats, liste des actions)\n",
    "    \"\"\"\n",
    "    solutions_list = []\n",
    "    # liste des etats\n",
    "    states_list = []\n",
    "    # liste des actions\n",
    "    actions_list=[]\n",
    "    for i in range(nb_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        total_reward, r, states_list, actions_list = Qlearn(i)\n",
    "        #memoriser si succes        \n",
    "        if r==1 :\n",
    "            solutions_list.append((i, total_reward, states_list, actions_list))\n",
    "    #derniere solution trouvee : \n",
    "    print(\"derniere solution trouvee : \")\n",
    "    print(\"--> etape no : \", solutions_list[-1][0])\n",
    "    print(\"--> valeur : \", solutions_list[-1][1])\n",
    "    print(\"--> nb actions : \", len(solutions_list[-1][2]))\n",
    "    print(\"--> les actions : \", end=\"\")\n",
    "    for a in solutions_list[-1][3]:\n",
    "        print(actions[a], end=\", \")\n",
    "    print()\n",
    "    return solutions_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derniere solution trouvee : \n",
      "--> etape no :  49667\n",
      "--> valeur :  0.976\n",
      "--> nb actions :  25\n",
      "--> les actions : Bas, Droite, Droite, Droite, Bas, Droite, Droite, Droite, Bas, Bas, Bas, Bas, Bas, Gauche, Bas, Gauche, Bas, Droite, Droite, Droite, Droite, Droite, Droite, Droite, Droite, \n"
     ]
    }
   ],
   "source": [
    "solutions = try_qlearn()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage de l'environnement\n",
    "Affichons maintenant la liste des actions via l'environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendu(states_list, actions_list):\n",
    "    \"\"\" affiche la liste des etats rencontres et les actions qui les ont causes\n",
    "    par le rendu de gym\n",
    "    Parameters\n",
    "    ----------\n",
    "    states_list : liste des etats\n",
    "    actions_list : liste des actions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for i in range(0, len(states_list)):\n",
    "        env.env.s = states_list[i]\n",
    "        print(\"action \", actions[actions_list[i]])\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rendu des actions, rappel le terrain est glissant....\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFF\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Gauche\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Gauche\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "print(\"rendu des actions, rappel le terrain est glissant....\")\n",
    "rendu(solutions[-1][ 2], solutions[-1][ 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Traçons une courbe pour évaluer la progression de l'apprentissage entre chaque test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_qlearn(solutions_list):\n",
    "    \"\"\"\n",
    "    dessine l'evolution des recompenses des solutions trouvees\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions_list : liste des solutions\n",
    "    \"\"\"\n",
    "    xs = [x[0] for x in solutions_list]\n",
    "    ys = [y[1] for y in solutions_list]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(xs, ys, '.')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_qlearn(solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les points sont de plus en plus rapprochés mais toujours disparates, ce ne serait le cas si l'environnement n'était stochastique.\n",
    "\n",
    "Tentez en modifiant plus haut le chargement de l'environnement : \n",
    "\n",
    "`env = gym.make('FrozenLake8x8-v0', is_slippery=False)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tentez aussi d'implémenter le ***double Q Learning*** pour étudier le gain..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
