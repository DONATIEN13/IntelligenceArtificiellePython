{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/EmmanuelADAM/IntelligenceArtificiellePython/blob/master/TPMLGymQLearning-Solution.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "## Appliqué à [Gym.OpenAI](https://gym.openai.com)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Présentation de Gym\n",
    "\n",
    "Voir la page d'introduction à [Gym](https://gym.openai.com)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation de gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outil AUTRE QUE COLAB (pyzo, jupyter lab, .....)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Test de ML par Q-Learning pour atteindre l'objectif\n",
    "- Utiliser l'environnement `FrozenLake8x8-v0` (un labyrinthe en mode texte)\n",
    "- 4 actions sont possibles (Left(0), Down(1), Right(2), Up(3))\n",
    "  - l'adjectif \"Frozen\" signifie qu'une *action n'est pas déterministe !*\n",
    "    - à partir d'une case \"gelée\", aller à droite peut .. mener à droite, ou pas\n",
    "    - => intérêt du Q-Learning adapté à ce type d'environnement probabiliste\n",
    "- Le labyrinthe est ainsi composé de zones glacées, de puits, et d'un objectif\n",
    "\n",
    "\n",
    "**N.B.** \n",
    "  - *Cet environnement fonctionne bien sous colab, jupyterlab.. quelques soucis de l'affichage de l'état courant (carré rouge) sous Pyzo....* \n",
    "  - Il est fortement conseillé de débuter avec un environnement déterministe pour évaluer la bonne marche de l'algo de Q-Learning que vous aurez développer.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Etude de l'environnement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "specification de l'environnement :  EnvSpec(FrozenLake-v0)\n",
      "espace d'actions :  Discrete(4)  => 4 actions \"discretes\" (non continues)\n",
      "espace d'etats :  Discrete(16)  => 16 etats distincts\n",
      "Environnement et etat initial (en rouge) : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "S = Start (pos 0), G = Goal (pos 15), H = Hole, F = Frozen place\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False) # tester FrozenLake8x8 pour l'environnement plus large\n",
    "print(\"specification de l'environnement : \", env.spec)\n",
    "print(\"espace d'actions : \", env.action_space , \" => 4 actions \\\"discretes\\\" (non continues)\") #ici 4 actions discrétisée\n",
    "print(\"espace d'etats : \", env.observation_space , \" => 16 etats distincts\") #ici 4x4 cellules possibles\n",
    "\n",
    "env.reset()\n",
    "print(\"Environnement et etat initial (en rouge) : \")\n",
    "env.render()\n",
    "print(\"S = Start (pos 0), G = Goal (pos 15), H = Hole, F = Frozen place\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "#### Test des actions\n",
    "\n",
    "Sous Gym, `step` permet d'effectuer une action. \n",
    "En retour la fonction retourne une observation sur l'etat d'arrivee, sa recompense, son type (final ou non), et des informations.\n",
    "Ici, dans FrozenLake, \n",
    "- observation = position où se trouve l'agent\n",
    "- reward = recompense\n",
    "- done = vrai si but atteint\n",
    "- info = probabilité de succès de l'action \n",
    "  - en mode déterministe, sol non glissant, la proba de réussite est de 100%\n",
    "  - en mode non déterministe, sol glissant, la proba de réussite est de 30%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "###### Test des actions\n",
    "env.reset()\n",
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "\u001b[41mF\u001b[0mHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 4 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 0 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "action = 3\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Cas non déterministe**\n",
    "\n",
    "L'environnement FrozenLake peut également être chargé en mode non déterministe : chaque état est une case gelée, et chaque action qui s'y deroule n'a qu'une chance sur trois de réussir !\n",
    "\n",
    "Chargeons l'environnement dans ce mode et testons les actions à partir de l'état initial : "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Left)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 5 ,gain: 0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=True) \n",
    "\n",
    "env.reset()\n",
    "env.env.s = 5\n",
    "action = 0\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 5 ,gain: 0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 5\n",
    "action = 1\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 5 ,gain: 0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 5\n",
    "action = 2\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Up)\n",
      "SFFF\n",
      "F\u001b[41mH\u001b[0mFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 5 ,gain: 0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env.reset()\n",
    "env.env.s = 5\n",
    "action = 3\n",
    "observation, reward, done, info = env.step(action)\n",
    "env.render()\n",
    "print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On est clairement ici dans un environnement non déterministe (une même action à partir d'un même état ne mène pas toujours au même résultat); c'est le contexte de prédilection de l'algo de Q-Learning..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### <font color=\"red\">Premiere résolution en mode déterministe</font>\n",
    "Important, pour valider l'apprentissage de votre algorithme avant de passer en mode non-déterministe, il vaut mieux le tester sur un environnement où chaque action à 100% de réussite. Ci-dessous un exemple sur le mini labyrinthe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Right)\n",
      "S\u001b[41mF\u001b[0mFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 1 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SF\u001b[41mF\u001b[0mF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 2 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FH\u001b[41mF\u001b[0mH\n",
      "FFFH\n",
      "HFFG\n",
      "pos° actuelle: 6 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FF\u001b[41mF\u001b[0mH\n",
      "HFFG\n",
      "pos° actuelle: 10 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HF\u001b[41mF\u001b[0mG\n",
      "pos° actuelle: 14 ,gain: 0.0 ,fini: False , {'prob': 1.0}\n",
      "  (Right)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "pos° actuelle: 15 ,gain: 1.0 ,fini: True , {'prob': 1.0}\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLake-v0', is_slippery=False)\n",
    "env.reset()\n",
    "actions = [2,2,1,1,1,2]\n",
    "for a in actions:\n",
    "    observation, reward, done, info = env.step(a)\n",
    "    env.render()\n",
    "    print(\"pos° actuelle:\", observation,\",gain:\", reward,\",fini:\", done,\",\", info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Exemple d'algorithme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rnd\n",
    "\n",
    "env = gym.make('FrozenLake8x8-v0', is_slippery=False)\n",
    "actions = {0:'Gauche', 1:'Bas', 2:'Droite', 3:'Haut'}\n",
    "\n",
    "# initialiser la Q-Table\n",
    "# autant de cases que l'environnement en possède, \n",
    "# contenant autant de valeurs que d'actions possibles\n",
    "# donc ici une matrice 64 x 4\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mise en place des paramètres\n",
    "Pour rappel l'algo de Q Learning simple repose sur cette équation : \n",
    "$Q(s,a) \\gets \\lambda \\times (r + \\gamma \\times max_{a'}(Q(s', a'))) + (1-\\lambda ) \\times Q(s,a)$ avec \n",
    "  - $\\lambda$ : coef d'apprentissage\n",
    "  - $\\gamma$ : coef de réduction \n",
    "  - $r$ : récompense\n",
    "  \n",
    "Cette équation donne la qualité de l'action *a* à partir de l'état *s*.\n",
    "\n",
    "Initialement, les actions sont choisies aléatoirement et notées; puis au fil des tests les actions les plus valuées sont choisies. Pour cela, un tirage est effectuée, s'il est inférieur à un $\\epsilon$, le choix est aléatoire. Cet $\\epsilon$ décroit au fil des tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_learn = .8\n",
    "gamma = 0.99\n",
    "epsilon = 1.\n",
    "nb_episodes = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Récupérer la meilleure action\n",
    "`argmax(tab)` retourne l'indice de la plus grande valeur du tableau.\n",
    "\n",
    "`argmax(Q[2])` retourne donc le no de l'action la plus intéressante à partir de l'état 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L'algorithme de Q-Learning simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "##algorithme de Q-Learning simple\n",
    "def q_learn():\n",
    "    \"\"\"\n",
    "    effectue un cycle d'apprentissage/recherche de solution' via le Q-Learning simple\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    epoch : no de l'etape\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    total_r : recompense totale\n",
    "    r : recompense du dernier etat rencontre\n",
    "    states_list : liste des etats traverses\n",
    "    actions_list : liste des actions effectuees\n",
    "\n",
    "    \"\"\"\n",
    "    s = env.reset()\n",
    "    total_r = 0\n",
    "    done = False\n",
    "    step = 0\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    # The Q-Table learning algorithm\n",
    "    while not done and step < 64:\n",
    "        step += 1\n",
    "        actions = Q[s, :]\n",
    "        # Choose random action if initial step or if there is no interesting action\n",
    "        if rnd.random()<epsilon or np.max(actions)==0:\n",
    "            a = rnd.randint(0, env.action_space.n-1)\n",
    "        else:\n",
    "            a = np.argmax(actions)\n",
    "\n",
    "        # Get new state and reward from environment\n",
    "        new_state, r, done, _ = env.step(a)\n",
    "\n",
    "        # Q-Learning\n",
    "        Q[s, a] = Q[s, a] + lambda_learn*(r + gamma * np.max(Q[new_state, :]) - Q[s, a])\n",
    "        s = new_state\n",
    "        total_r = total_r + r\n",
    "        states_list.append(s)\n",
    "        actions_list.append(a)\n",
    "    return total_r, r, states_list, actions_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def try_qlearn():\n",
    "    \"\"\"\n",
    "    lance nb_episodes fois un cycle de Q-Learning et memorise chaque solution trouvee\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    solutions_list : liste des solutions (no, recompense totale, liste des etats, liste des actions)\n",
    "    \"\"\"\n",
    "    global epsilon\n",
    "    states_list = []\n",
    "    actions_list = []\n",
    "    solutions_list = []\n",
    "    epsilon = 1\n",
    "    for i in range(nb_episodes):\n",
    "        # Reset environment and get first new observation\n",
    "        total_r, r, states_list, actions_list = q_learn()\n",
    "        epsilon = epsilon * 0.999\n",
    "        # memorize if a solution has been found\n",
    "        if r == 1: solutions_list.append((i, total_r, states_list, actions_list))\n",
    "        \n",
    "    if(len(solutions_list) == 0): print(\"aucune solution trouvee !!\")\n",
    "\n",
    "    return solutions_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Affichage de du résultat\n",
    "Affichons maintenant la liste des actions via l'environnement Gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rendu(solutions_list):\n",
    "    \"\"\" affiche la plus courte sequence d'actions permettant d'atteindre l'objectif q partir des solutions fournies\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions_list : liste des solutions trouvees\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "    \"\"\"\n",
    "    mini_sol = solutions_list[0]\n",
    "    for s in  solutions_list:\n",
    "        if len(s[2]) < len(mini_sol[2]): mini_sol = s\n",
    "    print(\"une solution en \", len(mini_sol[2]), \" etapes : \")\n",
    "    env.reset()\n",
    "    env.render()\n",
    "    for i in range(0, len(mini_sol[2])):\n",
    "        env.env.s = mini_sol[2][i]\n",
    "        print(\"action \", actions[mini_sol[3][i]])\n",
    "        env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une solution en  14  etapes : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "\u001b[41mF\u001b[0mFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFF\u001b[41mF\u001b[0mHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFH\u001b[41mF\u001b[0mFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHF\u001b[41mF\u001b[0mFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFF\u001b[41mF\u001b[0mF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "##ON LANCE LA RESOLUTION : \n",
    "solutions = try_qlearn()\n",
    "if(len(solutions)>0):rendu(solutions)\n",
    "#relancer le bloc si pas de solution trouvee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le gain est intéressant. Si on doit parcrourir tout l'arbre de recherche, la complexité de l'arbre est borné par $3 \\times 3 \\times \\dots \\times 3 = 3^{63} = 1 144 561 273 430 837 494 885 949 696 427$ solutions à balayer.\n",
    "\n",
    "Ici, $1000 \\times 25$ actions on été testées..\n",
    "\n",
    "Traçons une courbe pour évaluer la progression de l'apprentissage entre chaque test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "def plot_frequence_sol(solutions_list):\n",
    "    \"\"\"\n",
    "    dessine la frequence de solution trouvees\n",
    "    Parameters\n",
    "    ----------\n",
    "    solutions : liste des solutions\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None.\n",
    "\n",
    "    \"\"\"\n",
    "    xs = [x[0] for x in solutions_list]\n",
    "    ys = [y[1] for y in solutions_list]\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.plot(xs, ys, '.')\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_frequence_sol(solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <font color=\"red\">Test de résolution en mode non déterministe</font>\n",
    "Rechargeons l'environnement en mode \"glissant\".\n",
    "\n",
    "Il suffit de réinitialiser la table Q et de lancer l'algorithme...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une solution en  20  etapes : \n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "\u001b[41mS\u001b[0mFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "S\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "F\u001b[41mF\u001b[0mFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SFFFFFFF\n",
      "FF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SF\u001b[41mF\u001b[0mFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFF\u001b[41mF\u001b[0mFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Bas\n",
      "\n",
      "SFFF\u001b[41mF\u001b[0mFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFF\u001b[41mF\u001b[0mFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFF\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Gauche\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHF\u001b[41mF\u001b[0m\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Gauche\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFH\u001b[41mF\u001b[0mF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFF\u001b[41mF\u001b[0mF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Haut\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFF\u001b[41mF\u001b[0m\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFH\u001b[41mF\u001b[0m\n",
      "FHFFHFHF\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFH\u001b[41mF\u001b[0m\n",
      "FFFHFFFG\n",
      "action  Droite\n",
      "\n",
      "SFFFFFFF\n",
      "FFFFFFFF\n",
      "FFFHFFFF\n",
      "FFFFFHFF\n",
      "FFFHFFFF\n",
      "FHHFFFHF\n",
      "FHFFHFHF\n",
      "FFFHFFF\u001b[41mG\u001b[0m\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('FrozenLake8x8-v0', is_slippery=True)\n",
    "\n",
    "env.reset()\n",
    "Q = np.zeros([env.observation_space.n,env.action_space.n])\n",
    "lambda_learn = .8\n",
    "gamma = 0.99\n",
    "epsilon = 1.\n",
    "nb_episodes = 5000\n",
    "##ON LANCE LA RESOLUTION : \n",
    "solutions = try_qlearn()\n",
    "if(len(solutions)>0):rendu(solutions)\n",
    "plot_frequence_sol(solutions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus on relance les tests, plus on a de chance de trouver une solution..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
